# ğŸ“š Fellows Curriculum: ì²´ê³„ì  ì—°êµ¬ ì—­ëŸ‰ ê°œë°œ

## í•™ìŠµ ë¡œë“œë§µ

---

## 1. êµìœ¡ ê³¼ì • ê°œìš”

### 1.1 ì»¤ë¦¬í˜ëŸ¼ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Connectome Fellows Curriculum                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Year 1: Foundation Building                                 â”‚
â”‚  â”œâ”€â”€ Module 1: Computational Neuroscience Basics            â”‚
â”‚  â”œâ”€â”€ Module 2: Deep Learning for Brain Science              â”‚
â”‚  â”œâ”€â”€ Module 3: Brain Data Analysis                          â”‚
â”‚  â””â”€â”€ Module 4: Research Methods & Ethics                    â”‚
â”‚                                                              â”‚
â”‚  Year 2: Advanced Research                                   â”‚
â”‚  â”œâ”€â”€ Module 5: Foundation Models                            â”‚
â”‚  â”œâ”€â”€ Module 6: Multimodal Learning                          â”‚
â”‚  â”œâ”€â”€ Module 7: Generative Models                            â”‚
â”‚  â””â”€â”€ Module 8: Scientific Writing                           â”‚
â”‚                                                              â”‚
â”‚  Year 3: Independent Research                               â”‚
â”‚  â”œâ”€â”€ Independent Project                                    â”‚
â”‚  â”œâ”€â”€ Thesis/Publication                                     â”‚
â”‚  â””â”€â”€ Mentorship of Junior Fellows                           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Year 1: Foundation Building

### Module 1: Computational Neuroscience Basics (Month 1-2)

#### í•™ìŠµ ëª©í‘œ
- ë‡Œì˜ ê¸°ë³¸ êµ¬ì¡°ì™€ ê¸°ëŠ¥ ì´í•´
- ì‹ ê²½ ì‹ í˜¸ì˜ ì›ë¦¬ í•™ìŠµ
- ì£¼ìš” ë‡Œì˜ìƒ ê¸°ë²• ì´í•´

#### ì»¤ë¦¬í˜ëŸ¼

| ì£¼ì°¨ | ì£¼ì œ | ìë£Œ | ê³¼ì œ |
|------|------|------|------|
| 1 | ì‹ ê²½ê³¼í•™ ê°œë¡  | Kandel Ch.1-3 | Quiz |
| 2 | ë‰´ëŸ°ê³¼ ì‹œëƒ…ìŠ¤ | Kandel Ch.4-6 | ë¦¬ë·° ì‘ì„± |
| 3 | ê°ê° ì‹œìŠ¤í…œ | Kandel Ch.21-23 | ë°œí‘œ |
| 4 | ì¸ì§€ì™€ ê¸°ì–µ | Kandel Ch.66-67 | í”„ë¡œì íŠ¸ |
| 5 | fMRI ì›ë¦¬ | Huettel Ch.1-4 | ì‹¤ìŠµ |
| 6 | EEG/MEG ì›ë¦¬ | Luck Ch.1-3 | ì‹¤ìŠµ |
| 7 | ë‡Œ ì—°ê²°ì„± | Sporns Ch.1-3 | ë¶„ì„ ì‹¤ìŠµ |
| 8 | ì¢…í•© ë° í‰ê°€ | - | ì‹œí—˜ |

#### í•„ìˆ˜ ì½ê¸°
```
í•„ë…ì„œ:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Kandel, "Principles of Neural Science" (ì„ íƒ ì¥)
2. Huettel, "Functional Magnetic Resonance Imaging"
3. Luck, "An Introduction to the Event-Related Potential Technique"
4. Sporns, "Networks of the Brain"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### Module 2: Deep Learning for Brain Science (Month 3-4)

#### í•™ìŠµ ëª©í‘œ
- PyTorch í”„ë ˆì„ì›Œí¬ ìˆ™ë‹¬
- ì£¼ìš” ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì²˜ ì´í•´
- ë‡Œ ë°ì´í„°ì— ì ìš©í•˜ëŠ” ë°©ë²• í•™ìŠµ

#### ì»¤ë¦¬í˜ëŸ¼

| ì£¼ì°¨ | ì£¼ì œ | ì‹¤ìŠµ | í”„ë¡œì íŠ¸ |
|------|------|------|----------|
| 1 | PyTorch Basics | MNIST | - |
| 2 | CNNs | Brain Image Classification | Mini-project |
| 3 | RNNs/LSTMs | EEG Sequence Modeling | - |
| 4 | Transformers | Attention Mechanisms | Mini-project |
| 5 | Self-Supervised Learning | Contrastive Learning | - |
| 6 | Vision Transformers | ViT for Brain Images | - |
| 7 | Foundation Models | BrainLM ì†Œê°œ | Final project |
| 8 | Project Presentations | - | ë°œí‘œ |

#### ì‹¤ìŠµ ì½”ë“œ

```python
# Week 1-2: Basic PyTorch for Brain Classification
import torch
import torch.nn as nn
from torchvision import models

class BrainClassifier(nn.Module):
    """
    fMRI slice ê¸°ë°˜ ë‡Œ ìƒíƒœ ë¶„ë¥˜ê¸°
    """
    def __init__(self, num_classes=4):
        super().__init__()
        # Pre-trained ResNet backbone
        self.backbone = models.resnet18(pretrained=True)
        self.backbone.conv1 = nn.Conv2d(1, 64, 7, stride=2, padding=3)  # 1-channel input
        self.backbone.fc = nn.Linear(512, num_classes)
        
    def forward(self, x):
        return self.backbone(x)

# Training loop
def train_classifier(model, dataloader, optimizer, criterion):
    model.train()
    for batch_idx, (data, target) in enumerate(dataloader):
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
```

### Module 3: Brain Data Analysis (Month 5-6)

#### í•™ìŠµ ëª©í‘œ
- fMRI ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- EEG ì‹ í˜¸ ë¶„ì„ ê¸°ë²• ìŠµë“
- í†µê³„ì  ë¶„ì„ ë°©ë²• í•™ìŠµ

#### ì‹¤ìŠµ í™˜ê²½ ì„¤ì •

```python
# Brain Data Analysis Environment
required_packages = [
    # fMRI Analysis
    "nilearn>=0.10.0",
    "nibabel>=5.0.0",
    "fslpy>=3.0.0",
    
    # EEG Analysis
    "mne>=1.5.0",
    "mne-bids>=0.13",
    
    # Statistics
    "scipy>=1.11.0",
    "statsmodels>=0.14.0",
    "pingouin>=0.5.3",
    
    # Visualization
    "matplotlib>=3.8.0",
    "seaborn>=0.13.0",
    "plotly>=5.18.0",
]
```

#### fMRI ë¶„ì„ ì‹¤ìŠµ

```python
# fMRI Preprocessing Pipeline
from nilearn import datasets, image, masking
from nilearn.glm.first_level import FirstLevelModel

class fMRIPipeline:
    """
    í‘œì¤€ fMRI ì „ì²˜ë¦¬ ë° ë¶„ì„ íŒŒì´í”„ë¼ì¸
    """
    
    def __init__(self, data_dir):
        self.data_dir = data_dir
        
    def preprocess(self, bold_img, confounds):
        """
        ì „ì²˜ë¦¬ ë‹¨ê³„:
        1. Motion correction
        2. Slice timing correction
        3. Spatial normalization
        4. Smoothing
        5. Confound regression
        """
        # Smoothing
        smoothed = image.smooth_img(bold_img, fwhm=6)
        
        # Masking
        mask = masking.compute_brain_mask(smoothed)
        masked_data = masking.apply_mask(smoothed, mask)
        
        # Confound regression
        cleaned = self.regress_confounds(masked_data, confounds)
        
        return cleaned
    
    def extract_rois(self, cleaned_data, atlas='schaefer_400'):
        """
        ROI ì‹œê³„ì—´ ì¶”ì¶œ
        """
        from nilearn import datasets
        
        if atlas == 'schaefer_400':
            atlas_data = datasets.fetch_atlas_schaefer_2018(n_rois=400)
        
        from nilearn.maskers import NiftiLabelsMasker
        masker = NiftiLabelsMasker(atlas_data.maps)
        time_series = masker.fit_transform(cleaned_data)
        
        return time_series
```

### Module 4: Research Methods & Ethics (Month 7-8)

#### í•™ìŠµ ëª©í‘œ
- ê³¼í•™ì  ì—°êµ¬ ë°©ë²•ë¡  ì´í•´
- ì—°êµ¬ ìœ¤ë¦¬ ë° ì±…ì„ ìˆëŠ” ì—°êµ¬ ìˆ˜í–‰
- ì¬í˜„ ê°€ëŠ¥í•œ ì—°êµ¬ ì‹¤ì²œ

#### ì£¼ìš” ì£¼ì œ

| ì£¼ì°¨ | ì£¼ì œ | í˜•ì‹ | ê³¼ì œ |
|------|------|------|------|
| 1 | ê³¼í•™ì  ë°©ë²•ë¡  | ê°•ì˜ | ì—°êµ¬ ì„¤ê³„ |
| 2 | ì‹¤í—˜ ì„¤ê³„ | ì›Œí¬ìˆ | IRB ì‘ì„± |
| 3 | í†µê³„ì  ì¶”ë¡  | ê°•ì˜ | ë¶„ì„ ì‹¤ìŠµ |
| 4 | ì—°êµ¬ ìœ¤ë¦¬ | ì„¸ë¯¸ë‚˜ | ì‚¬ë¡€ ë¶„ì„ |
| 5 | ë°ì´í„° ê´€ë¦¬ | ì‹¤ìŠµ | BIDS ë³€í™˜ |
| 6 | ì¬í˜„ì„± ìœ„ê¸° | í† ë¡  | ë…¼ë¬¸ ë¦¬ë·° |
| 7 | Open Science | ì›Œí¬ìˆ | GitHub ì‹¤ìŠµ |
| 8 | ì¢…í•© í‰ê°€ | - | ì—°êµ¬ ê³„íšì„œ |

---

## 3. Year 2: Advanced Research

### Module 5: Foundation Models (Month 1-3)

#### í•™ìŠµ ëª©í‘œ
- Foundation Model ê°œë… ì´í•´
- BrainLM, Brain-JEPA êµ¬í˜„
- Transfer learning ê¸°ë²• ìŠµë“

#### ì‹¬í™” í•™ìŠµ ë‚´ìš©

```python
# BrainLM Implementation Study
class BrainLMStudy:
    """
    BrainLM ë…¼ë¬¸ ì¬í˜„ ë° ì´í•´
    
    í•™ìŠµ ë‚´ìš©:
    1. Autoregressive pretraining
    2. Temporal attention
    3. Transfer to downstream tasks
    """
    
    topics = [
        {
            "week": 1,
            "topic": "Paper deep dive",
            "activity": "ë…¼ë¬¸ ì •ë… ë° ë°œí‘œ",
            "output": "ë°œí‘œ ìë£Œ"
        },
        {
            "week": 2,
            "topic": "Architecture implementation",
            "activity": "ì½”ë“œ êµ¬í˜„",
            "output": "GitHub PR"
        },
        {
            "week": 3,
            "topic": "Training pipeline",
            "activity": "í•™ìŠµ ì‹¤í—˜",
            "output": "ì‹¤í—˜ ë¡œê·¸"
        },
        {
            "week": 4,
            "topic": "Evaluation",
            "activity": "ì„±ëŠ¥ í‰ê°€",
            "output": "ë³´ê³ ì„œ"
        },
    ]
```

### Module 6: Multimodal Learning (Month 4-6)

#### í•µì‹¬ ë‚´ìš©
- Cross-modal alignment
- Contrastive learning (CLIP-style)
- Brain-Language alignment (Hasson Lab ì—°êµ¬)

#### í”„ë¡œì íŠ¸

```python
# Multimodal Brain-Language Alignment
class BrainLanguageProject:
    """
    fMRI-Language ì •ë ¬ í”„ë¡œì íŠ¸
    
    ë°ì´í„°: Narratives dataset (Hasson Lab)
    ëª©í‘œ: ì–¸ì–´ ìê·¹ê³¼ ë‡Œ í™œë™ì˜ ì •ë ¬ í•™ìŠµ
    """
    
    def __init__(self):
        self.language_encoder = "whisper-large"  # or LLM
        self.brain_encoder = "brainlm"
        
    def contrastive_alignment(self, text_embeddings, brain_embeddings):
        """
        InfoNCE lossë¡œ ì •ë ¬ í•™ìŠµ
        """
        # Normalize
        text_norm = F.normalize(text_embeddings, dim=-1)
        brain_norm = F.normalize(brain_embeddings, dim=-1)
        
        # Similarity matrix
        logits = torch.matmul(text_norm, brain_norm.T) / self.temperature
        
        # Contrastive loss
        labels = torch.arange(len(logits), device=logits.device)
        loss = (F.cross_entropy(logits, labels) + 
                F.cross_entropy(logits.T, labels)) / 2
        
        return loss
```

### Module 7: Generative Models (Month 7-9)

#### í•™ìŠµ ë‚´ìš©
- VAE for brain data
- Diffusion models
- Brain activity generation

### Module 8: Scientific Writing (Month 10-12)

#### í•™ìŠµ ëª©í‘œ
- í•™ìˆ  ë…¼ë¬¸ ì‘ì„±ë²•
- í•™íšŒ ë°œí‘œ ê¸°ìˆ 
- í”¼ì–´ ë¦¬ë·° ì°¸ì—¬

#### ê¸€ì“°ê¸° ì›Œí¬ìˆ

| ì£¼ì°¨ | ì£¼ì œ | ê³¼ì œ |
|------|------|------|
| 1-2 | Introduction ì‘ì„± | ì—°êµ¬ ë°°ê²½ ì´ˆì•ˆ |
| 3-4 | Methods ì‘ì„± | ë°©ë²•ë¡  ìƒì„¸ ê¸°ìˆ  |
| 5-6 | Results ì‘ì„± | ê²°ê³¼ ì‹œê°í™” |
| 7-8 | Discussion ì‘ì„± | ë…¼ì˜ ë° ê²°ë¡  |
| 9-10 | Revision | í”¼ë“œë°± ë°˜ì˜ |
| 11-12 | Submission prep | ìµœì¢… ì œì¶œ ì¤€ë¹„ |

---

## 4. Technical Skills Roadmap

### 4.1 í”„ë¡œê·¸ë˜ë° ì—­ëŸ‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Programming Skills Progression                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Level 1: Beginner (Month 1-3)                              â”‚
â”‚  â”œâ”€â”€ Python basics, NumPy, Pandas                          â”‚
â”‚  â”œâ”€â”€ Matplotlib, Seaborn visualization                      â”‚
â”‚  â””â”€â”€ Jupyter notebooks                                      â”‚
â”‚                                                              â”‚
â”‚  Level 2: Intermediate (Month 4-6)                          â”‚
â”‚  â”œâ”€â”€ PyTorch fundamentals                                   â”‚
â”‚  â”œâ”€â”€ Git/GitHub workflow                                    â”‚
â”‚  â””â”€â”€ Shell scripting, SLURM                                 â”‚
â”‚                                                              â”‚
â”‚  Level 3: Advanced (Month 7-12)                             â”‚
â”‚  â”œâ”€â”€ Distributed training                                   â”‚
â”‚  â”œâ”€â”€ Custom CUDA kernels (optional)                        â”‚
â”‚  â””â”€â”€ MLOps (Weights & Biases, MLflow)                      â”‚
â”‚                                                              â”‚
â”‚  Level 4: Expert (Year 2+)                                  â”‚
â”‚  â”œâ”€â”€ JAX/Flax for research                                 â”‚
â”‚  â”œâ”€â”€ Efficient attention mechanisms                         â”‚
â”‚  â””â”€â”€ Foundation model development                           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 ì‹ ê²½ê³¼í•™ ë„êµ¬

```python
neuro_tools_curriculum = {
    "fMRI": {
        "beginner": ["nilearn", "nibabel"],
        "intermediate": ["FSL", "AFNI", "FreeSurfer"],
        "advanced": ["fMRIPrep", "custom pipelines"]
    },
    "EEG/MEG": {
        "beginner": ["MNE-Python basics"],
        "intermediate": ["source localization", "connectivity"],
        "advanced": ["real-time processing", "BCI"]
    },
    "analysis": {
        "beginner": ["GLM", "t-tests", "correlation"],
        "intermediate": ["RSA", "encoding models", "MVPA"],
        "advanced": ["Bayesian modeling", "causal inference"]
    }
}
```

---

## 5. í‰ê°€ ì²´ê³„

### 5.1 í‰ê°€ ê¸°ì¤€

| êµ¬ë¶„ | ë¹„ì¤‘ | ë‚´ìš© |
|------|------|------|
| **Course Work** | 30% | ëª¨ë“ˆë³„ ê³¼ì œ, ì‹œí—˜ |
| **Research Progress** | 40% | ì—°êµ¬ í”„ë¡œì íŠ¸ ì§„í–‰ |
| **Presentation** | 15% | ì„¸ë¯¸ë‚˜, í•™íšŒ ë°œí‘œ |
| **Collaboration** | 15% | íŒ€ì›Œí¬, ë©˜í† ë§ |

### 5.2 ë§ˆì¼ìŠ¤í†¤

| ì‹œì  | ê¸°ëŒ€ ì„±ê³¼ |
|------|----------|
| **6ê°œì›”** | Module 1-3 ì™„ë£Œ, ê¸°ì´ˆ ì—°êµ¬ ì‹œì‘ |
| **12ê°œì›”** | Year 1 ì™„ë£Œ, ì²« í•™íšŒ ë°œí‘œ |
| **18ê°œì›”** | ë…ë¦½ í”„ë¡œì íŠ¸ ì§„í–‰ ì¤‘ |
| **24ê°œì›”** | ë…¼ë¬¸ ì´ˆê³  ì™„ì„± |
| **36ê°œì›”** | ë…¼ë¬¸ ì¶œíŒ, ëŒ€í•™ì› ì¤€ë¹„ ì™„ë£Œ |

---

## 6. ì¶”ì²œ ì˜¨ë¼ì¸ ê°•ì¢Œ

### 6.1 í•„ìˆ˜ ì½”ìŠ¤

| ê°•ì¢Œ | í”Œë«í¼ | ë‚œì´ë„ | ê¸°ê°„ |
|------|--------|--------|------|
| [Neuromatch Comp Neuro](https://compneuro.neuromatch.io) | NMA | ì¤‘ê¸‰ | 3ì£¼ |
| [Neuromatch Deep Learning](https://deeplearning.neuromatch.io) | NMA | ì¤‘ê¸‰ | 3ì£¼ |
| [CS231n (CNN)](http://cs231n.stanford.edu) | Stanford | ì¤‘ê¸‰ | 10ì£¼ |
| [CS224n (NLP)](http://cs224n.stanford.edu) | Stanford | ì¤‘ê¸‰ | 10ì£¼ |
| [Full Stack Deep Learning](https://fullstackdeeplearning.com) | Berkeley | ê³ ê¸‰ | 8ì£¼ |

### 6.2 ì„ íƒ ì½”ìŠ¤

| ê°•ì¢Œ | ì£¼ì œ | ì¶”ì²œ ëŒ€ìƒ |
|------|------|----------|
| [Principles of fMRI](https://www.coursera.org/learn/functional-mri) | fMRI | ì˜ìƒ ê´€ì‹¬ì |
| [Neural Signal Processing](https://www.edx.org/) | EEG/MEG | BCI ê´€ì‹¬ì |
| [Bayesian Statistics](https://www.coursera.org/learn/bayesian-statistics) | í†µê³„ | ì´ë¡  ê´€ì‹¬ì |

---

## 7. í•™ìŠµ ìì›

### 7.1 í•µì‹¬ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸

```
Foundation Models:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. BrainLM (ICLR 2024)
2. Brain-JEPA (NeurIPS 2024)
3. Brain Harmony (NeurIPS 2025)
4. Foundation model of neural activity (Nature 2025)
5. fMRI-LM (arXiv 2025)

Brain-Language:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Shared computational principles (Hasson, Nat Neuro 2022)
2. BrainLLM (Nature Comms 2025)
3. Whisper for brain encoding (2025)

Representation Learning:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Platonic Representation Hypothesis (ICML 2024)
2. NeuroAI Turing Test (arXiv 2025)
3. Brain-model alignment studies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### 7.2 êµì¬

| êµì¬ | ì €ì | ìš©ë„ |
|------|------|------|
| Deep Learning | Goodfellow et al. | DL ê¸°ì´ˆ |
| Pattern Recognition and ML | Bishop | ML ì´ë¡  |
| Neuroimaging Analysis | Poldrack | fMRI |
| Theoretical Neuroscience | Dayan & Abbott | ì´ë¡  |

---

## 8. AI ë„êµ¬ í™œìš© ê°€ì´ë“œ

### 8.1 Coding Agent ì‚¬ìš©ë²•

```python
# Claude Code íš¨ê³¼ì  ì‚¬ìš© ê°€ì´ë“œ
claude_usage_guide = {
    "code_generation": {
        "best_practices": [
            "ëª…í™•í•œ docstring ìš”ì²­",
            "ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ í•¨ê»˜ ìƒì„±",
            "ì ì§„ì  ë³µì¡ë„ ì¦ê°€",
        ],
        "example_prompt": '''
        PyTorchë¡œ BrainLMì˜ temporal attention ëª¨ë“ˆì„ êµ¬í˜„í•´ì¤˜.
        - ì…ë ¥: (batch, time, features)
        - causal masking í¬í•¨
        - ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ë„ ì‘ì„±
        '''
    },
    "debugging": {
        "best_practices": [
            "ì—ëŸ¬ ë©”ì‹œì§€ ì „ì²´ ê³µìœ ",
            "ê´€ë ¨ ì½”ë“œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ",
            "ì‹œë„í•œ ë°©ë²• ì„¤ëª…",
        ]
    },
    "research_assistance": {
        "best_practices": [
            "ë…¼ë¬¸ ìš”ì•½ ìš”ì²­",
            "ë°©ë²•ë¡  ë¹„êµ ë¶„ì„",
            "ì‹¤í—˜ ì„¤ê³„ ë¸Œë ˆì¸ìŠ¤í† ë°",
        ]
    }
}
```

### 8.2 ì›”ê°„ AI ì‚¬ìš© ì˜ˆì‚° ê°€ì´ë“œ

| ìš©ë„ | ì¶”ì²œ ë„êµ¬ | ì˜ˆì‚° |
|------|----------|------|
| **ì½”ë“œ ì‘ì„±** | Claude Code, Cursor | $60 |
| **ë…¼ë¬¸ ë¶„ì„** | Claude Opus, GPT-5 | $150 |
| **ë¬¸í—Œ ê²€ìƒ‰** | Gemini Deep Research | $50 |
| **ë°°ì¹˜ ì²˜ë¦¬** | DeepSeek-R1 | $20 |
| **ì‹¤í—˜ ê´€ë¦¬** | Weights & Biases | $20 |
| **ê³„ì‚°** | Cloud credits | $200 |

---

*Document Version: 1.0*
*Last Updated: December 2025*

